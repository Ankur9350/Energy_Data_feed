# -*- coding: utf-8 -*-
"""ieaorg_scrapping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r2d-OzQG4rOPVSMNybB_tAyl3M8v745I
"""

import sys
import json
import time
from bs4 import BeautifulSoup
import requests
import pandas as pd
import datetime
!pip install transformers
from transformers import pipeline

import bs4
collections=[]
def get_html(url):
    r = requests.get(url)
    return r.text
def get_soup(html):
    soup = bs4.BeautifulSoup(html, 'html.parser')
    return soup

def change_date(date):
  obj = datetime.datetime.strptime(date, "%d %B %Y")
  return obj.strftime("%Y-%m-%d")

pagesToGet=1
posts=[]
url='https://iea.org/news??tid=All&page='
for page in range(0,pagesToGet+1):
  try:
    print('Processing Page :', url+str(page))
    try:
      page=requests.get(url+str(page))

    except Exception as e:
      error_type,error_obj,error_info= sys.exc_info()
      print('Error For Link:', url)
      print(error_type,'Line:', error_info().tb_lineno)
      continue


    summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
    soup= BeautifulSoup(page.content , 'html.parser')
    articles = soup.findAll('article')
    print("Article Found : ",len(articles))
    for article in articles:
        print("--------")
        link = "https://iea.org" + article.find('a',{'class':'m-news-detailed-listing__link'}).get('href')
        title = article.find('span',{'class':'m-news-detailed-listing__hover'}).text
        date = change_date(article.find('div',{'class':'m-news-detailed-listing__date'}).text.strip())
        print("Scraping : ",link)
        article_html = get_html(link)
        article_soup = BeautifulSoup(article_html, 'html.parser')
        desc = article_soup.find('div',{'class':'f-rte--block'}).text.lstrip('\n')
        sum = summarizer(desc, max_length=150, min_length=50, do_sample=False)

        posts.append({'publish_date': date, 'source': "IEA", 'title': title, 'description':desc, 'summary' : sum})
  except Exception as e:
    print("Exception :",e)
    continue

posts

with open('posts.json',"w") as file:
  json.dump(posts,file,indent=5)

from google.colab import files
files.download('posts.json')

from google.colab import drive
drive.mount('/content/drive')